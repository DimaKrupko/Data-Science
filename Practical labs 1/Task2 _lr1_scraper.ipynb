{"cells":[{"cell_type":"markdown","metadata":{"id":"JKinrAikM30x"},"source":["## APIs and Web Scraping"]},{"cell_type":"markdown","metadata":{"id":"OwJ7uwILM305"},"source":["## Working with APIs\n","\n","Since everyone loves food (presumably), the ultimate end goal of this homework will be to acquire the data to answer some questions and hypotheses about the restaurant scene in Pittsburgh (which we will get to later). We will download __both__ the metadata on restaurants in Pittsburgh from the Yelp API and with this metadata, retrieve the comments/reviews and ratings from users on restaurants.\n","\n","But first things first, let's do the \"hello world\" of making web requests with Python to get a sense for how to programmatically access web pages: an (unauthenticated) HTTP GET to download a web page.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"bWEyb06iM303"},"source":["## Introduction\n","\n","While many people might view working with data (including scraping, parsing, storing, etc.) a necessary evil to get to the \"fun\" stuff (i.e. modeling), we think that if presented in the right way this munging can be quite empowering. Imagine you never had to worry or ask those _what if_ questions about data existing or being accessible... but that you can get it yourself!\n","\n","### Objectives\n","\n","More concretely, this labwork will teach you (and test you on):\n","\n","* HTTP Requests (and lifecycle)\n","* RESTful APIs\n","    * Authentication (OAuth)\n","    * Pagination\n","    * Rate limiting\n","* JSON vs. HTML (and how to parse each)\n","* HTML traversal (CSS selectors)\n","\n","### Library Documentation\n","\n","* Standard Library:\n","    * [io](https://docs.python.org/2/library/io.html)\n","    * [time](https://docs.python.org/2/library/time.html)\n","    * [json](https://docs.python.org/2/library/json.html)\n","\n","* Third Party\n","    * [requests](http://docs.python-requests.org/en/master/)\n","    * [Beautiful Soup (version 4)](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n","    * [yelp-fusion](https://www.yelp.com/developers/documentation/v3/get_started)\n","\n","**Note:** You may come across a `yelp-python` library online. The library is deprecated and incompatible with the current Yelp API, so do not use the library."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702850676626,"user":{"displayName":"Дмитро Степанович Крупко","userId":"11666038436664773564"},"user_tz":-120},"id":"yfE52NW3M306"},"outputs":[],"source":["# setup library imports\n","import io, time, json\n","import requests\n","from bs4 import BeautifulSoup\n","import re"]},{"cell_type":"markdown","metadata":{"id":"5aWsmdzhM307"},"source":["## Q0: Basic HTTP Requests\n","\n","Fill in the funtion to use `requests` to download and return the raw HTML content of the URL passed in as an argument. As an example try the following NYT article (on Facebook's algorithmic news feed): [http://www.nytimes.com/2016/08/28/magazine/inside-facebooks-totally-insane-unintentionally-gigantic-hyperpartisan-political-media-machine.html](http://www.nytimes.com/2016/08/28/magazine/inside-facebooks-totally-insane-unintentionally-gigantic-hyperpartisan-political-media-machine.html)\n","\n","> Your function should return a tuple of: (`<status_code>`, `<raw_html>`)\n","\n","```python\n",">>> facebook_article = retrieve_html('http://www.nytimes.com/2016/08/28/magazine/inside-facebooks-totally-insane-unintentionally-gigantic-hyperpartisan-political-media-machine.html')\n",">>> print(facebook_article)\n","(200, u'<!DOCTYPE html>\\n<!--[if (gt IE 9)|!(IE)]> <!--> <html lang=\"en\" class=\"no-js section-magazine...')\n","```"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":421,"status":"ok","timestamp":1702850686623,"user":{"displayName":"Дмитро Степанович Крупко","userId":"11666038436664773564"},"user_tz":-120},"id":"3Qqp4xVgM307","outputId":"b4faa4c9-217c-42ab-8030-1037dbaea9ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Status Code: 200\n","Raw HTML Content: <!DOCTYPE html>\n","<html lang=\"en\" class=\" story nytapp-vi-article\"  xmlns:og=\"http://opengraphprotocol.org/schema/\">\n","  <head>\n","    <meta charset=\"utf-8\" />\n","    <title data-rh=\"true\">Inside Facebook’s (To\n"]}],"source":["# import requests\n","\n","def retrieve_html(url):\n","    \"\"\"\n","    Return the raw HTML at the specified URL.\n","\n","    Args:\n","        url (string): The URL to fetch the HTML from.\n","\n","    Returns:\n","        status_code (integer): The HTTP status code of the response.\n","        raw_html (string): The raw HTML content of the response, properly encoded according to the HTTP headers.\n","    \"\"\"\n","    # Define headers to mimic a legitimate browser request\n","    headers = {\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n","    }\n","\n","    # Send an HTTP GET request to the specified URL with headers\n","    response = requests.get(url, headers=headers)\n","    \n","    # Extract the HTTP status code and raw HTML content\n","    status_code = response.status_code\n","    raw_html = response.text\n","    \n","    # Return a tuple containing the status code and raw HTML content\n","    return status_code, raw_html\n","\n","url = \"http://www.nytimes.com/2016/08/28/magazine/inside-facebooks-totally-insane-unintentionally-gigantic-hyperpartisan-political-media-machine.html\"\n","status_code, raw_html = retrieve_html(url)\n","\n","# Print the results\n","print(\"Status Code:\", status_code)\n","print(\"Raw HTML Content:\", raw_html[:200])"]},{"cell_type":"markdown","metadata":{"id":"cFwevIjtM308"},"source":["Now while this example might have been fun, we haven't yet done anything more than we could with a web browser. To really see the power of programmatically making web requests we will need to interact with a API. For the rest of this homework we will be working with the [Yelp API](https://www.yelp.com/developers/documentation/v3/get_started) and Yelp data (for an extensive data dump see their [Academic Dataset Challenge](https://www.yelp.com/dataset_challenge)). The reasons for using the Yelp API are 3 fold:\n","\n","1. Incredibly rich dataset that combines:\n","    * entity data (users and businesses)\n","    * preferences (i.e. ratings)\n","    * geographic data (business location and check-ins)\n","    * temporal data\n","    * text in the form of reviews\n","    * and even images.\n","2. Well [documented API](https://www.yelp.com/developers/documentation/v3/get_started) with thorough examples.\n","3. Extensive data coverage so that you can find data that you know personally (from your home town/city or account). This will help with understanding and interpreting your results."]},{"cell_type":"markdown","metadata":{"id":"dkWuEdTpM309"},"source":["## Authentication\n","\n","To access the Yelp API however we will need to go through a few more steps than we did with the first NYT example. Most large web scale companies use a combination of authentication and rate limiting to control access to their data to ensure that everyone using it abides. The first step (even before we make any request) is to setup a Yelp account if you do not have one and get API credentials."]},{"cell_type":"markdown","metadata":{"id":"RgTGso-KM30-"},"source":["Now that we have our accounts setup we can start making requests! There are various authentication schemes that APIs use, including:\n","\n","* No authentication\n","* [HTTP basic authentication](https://en.wikipedia.org/wiki/Basic_access_authentication)\n","* Cookie based user login\n","* OAuth (v1.0 & v2.0, see this [post](http://stackoverflow.com/questions/4113934/how-is-oauth-2-different-from-oauth-1) explaining the differences)\n","* API keys\n","* Custom Authentication\n","\n","For the NYT example, since it is a publicly visible page we did not need to authenticate. HTTP basic authentication isn't too common for consumer sites/applications that have the concept of user accounts (like Facebook, LinkedIn, Twitter, etc.) but is simple to setup quickly and you often encounter it on with individual password protected pages/sites. I'm sure you have seen [this](http://i.stack.imgur.com/QnUZW.png) before somewhere.\n","\n","Cookie based user login is what the majority of services use when you login with a browser (i.e. username and password). Once you sign in to a service like Facebook, the response stores a cookie in your browser to remember that you have logged in (HTTP is stateless). Each subsequent request to the same domain (i.e. any page on `facebook.com`) also sends the cookie that contains the authentication information to remind Facebook's servers that you have already logged in.\n","\n","Many REST APIs however use OAuth (authentication using tokens) which can be thought of a programmatic way to \"login\" _another_ user. Using tokens, a user (or application) only needs to send the login credentials once in the initial authentication and as a response from the server gets a special signed token. This signed token is then sent in future requests to the server (in place of the user credentials).\n","\n","A similar concept common used by many APIs is to assign API Keys to each client that needs access to server resources. The client must then pass the API Key along with _every_ request it makes to the API to authenticate. This is because the server is typically relatively stateless and does not maintain a session between subsequent calls from the same client. Most APIs (including Yelp) allow you to pass the API Key via a special HTTP Header: `Authorization: Bearer <API_KEY>`. Check out the [docs](https://www.yelp.com/developers/documentation/v3/authentication) for more information.\n","\n","Yelp used to use OAuth tokens but has now switched to API Keys. **For the sake of backwards compatibility Yelp still provides a Client ID and Secret for OAuth, but you will not need those for this assignment.**\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"znzXL4x6M30_"},"source":["## Q1: Authenticated HTTP Request with the Yelp API\n","\n","Using the Yelp API, fill in the following function stub to make an authenticated request to the [search](https://www.yelp.com/developers/documentation/v3/business_search) endpoint.  Note that you'll need to pass your API key to the function, but you can store this any way you like (it's best practice to store the API key in a separate file, but fine to simply include it in your function for this simple example).\n","\n","When writing the python request, you'll need to pass in a custom header as well as a parameter. Here are some examples for [response headers](http://docs.python-requests.org/en/master/user/quickstart/#response-headers) and [passing parameters in urls](http://docs.python-requests.org/en/master/user/quickstart/#passing-parameters-in-urls)\n","\n","```python\n",">>> num_records, data = yelp_search('Pittsburgh')\n",">>> print(num_records)\n","240\n",">>> print(list(map(lambda x: x['name'], data)))\n","['Gaucho Parrilla Argentina', 'täkō', 'Noodlehead', \"Bae Bae's Kitchen\", \"DiAnoia's Eatery\", ...]\n","```\n","\n","(Note that there are of course more than 240 businesses in Pittsburgh, but the Yelp API will limit the count to return this amount.  More on this later)."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"16bUdGbQM31A"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of businesses: 2900\n","Business names:\n","['Gaucho Parrilla Argentina', 'täkō', 'Butcher and the Rye', 'Meat & Potatoes', \"DiAnoia's Eatery\", 'Fig & Ash Wood Fire Kitchen', 'Proper Brick Oven & Tap Room', 'Altius', \"Eddie V's Prime Seafood\", \"Bae Bae's Kitchen\", 'Poulet Bleu', 'Kaya', 'Gi-Jin', 'The Commoner', 'The Eagle Food & Beer Hall', 'Dish Osteria and Bar', \"Carmella's Plates & Pints\", 'Morcilla', 'Hofbrauhaus Pittsburgh', 'Muddy Waters Oyster Bar', 'Bridges & Bourbon', \"Nicky's Thai Kitchen\", 'Sienna Mercato', 'Eleven', \"Pusadee's Garden\", 'The Summit', 'Bakersfield', \"Alihan's Mediterranean Cuisine\", 'Fish Nor Fowl', 'Butterjoint', \"Emerson's\", 'Tupelo Honey Southern Kitchen & Bar', 'The Yard', 'Cioppino Seafood and Steakhouse', 'Alla Famiglia', 'Bonfire', 'Noodlehead', '40 North Restaurant & Bar', 'Siempre Algo', 'Monterey Bay Fish Grotto']\n"]}],"source":["def yelp_search(api_key, query):\n","    \"\"\"\n","    Make an authenticated request to the Yelp API.\n","\n","    Args:\n","        api_key (string): Your Yelp API key\n","        query (string): Search term\n","\n","    Returns:\n","        total (integer): Total number of businesses on Yelp corresponding to the query\n","        businesses (list): List of dicts representing each business\n","    \"\"\"\n","\n","    \n","    url = \"https://api.yelp.com/v3/businesses/search\"\n","\n","    \n","    headers = {\n","        'Authorization': f'Bearer {api_key}',\n","    }\n","\n","   \n","    params = {\n","        'location': query,\n","        'limit': 40,  \n","    }\n","\n","    # Make the request to the Yelp API\n","    response = requests.get(url, headers=headers, params=params)\n","\n","    \n","    if response.status_code == 200:\n","        # Parse the JSON response\n","        data = response.json()\n","        \n","        # Extract total number of businesses and the list of businesses\n","        total = data.get('total', 0)\n","        businesses = data.get('businesses', [])\n","\n","        return total, businesses\n","    else:\n","        # If the request was not successful, print the error code and response text\n","        print(f\"Error {response.status_code}: {response.text}\")\n","        return 0, []\n","\n","api_key = 'SncupRpadsRXrFICNWIGi-EVxGx48lDI8RtXn93qGrA17cDiMf2iXBJ9AbFaV9a0kwZh91X1EDLWUVKHOI2lk8c5eykO7IvUTvZEwVRnSKjgmvgL1w621sZ87nZ_ZXYx'\n","query = 'Pittsburgh'\n","\n","# Make the Yelp API request\n","num_records, data = yelp_search(api_key, query)\n","\n","# Print the results\n","print(\"Total number of businesses:\", num_records)\n","print(\"Business names:\")\n","print(list(map(lambda x: x['name'], data)))"]},{"cell_type":"markdown","metadata":{"id":"t5RAOs2IM31A"},"source":["Now that we have completed the \"hello world\" of working with the Yelp API, we are ready to really fly! The rest of the exercise will have a bit less direction since there are a variety of ways to retrieve the requested information but you should have all the component knowledge at this point to work with the API. Yelp being a fairly general platform actually has many more business than just restaurants, but by using the flexibility of the API we can ask it to only return the restaurants."]},{"cell_type":"markdown","metadata":{"id":"PuIhUUKvM31B"},"source":["## Parameterization and Pagination\n","\n","And before we can get any reviews on restaurants, we need to actually get the metadata on ALL of the restaurants in Pittsburgh. Notice above that while Yelp told us that there are more than 1000, the response contained far fewer actual `Business` objects. This is due to pagination and is a safeguard against returning __TOO__ much data in a single request (what would happen if there were 100,000 restaurants?) and can be used in conjuction with _rate limiting_ as well as a way to throttle and protect access to Yelp data.\n","\n","If an API has 1,000,000 records, but only returns 10 records per page and limits you to 5 requests per second... how long will it take to acquire ALL of the records contained in the API?\n","\n","One of the ways that APIs are an improvement over plain web scraping is the ability to make __parameterized__ requests. Just like the Python functions you have been writing have arguments (or parameters) that allow you to customize its behavior/actions (an output) without having to rewrite the function entirely, we can parameterize the queries we make to the Yelp API to filter the results it returns.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"SLWwXcwVM31B"},"source":["## Q2: Aquire all of the restaurants in Pittsburgh (on Yelp)\n","\n","Again using the [API documentation](https://www.yelp.com/developers/documentation/v3/business_search) for the `search` endpoint, fill in the following function to retrieve all of the _Restaurants_ (using the categories parameter)** for a given query, within a radius of 1500 meters.  To get the correct results here, **you will need to specificially issue the query with the `categories` parameter set to `restaurants` and the `radius` set to `1500`;**  If you fail to do these, your answers will not match those in our tests.\n","\n","You will need to account for __pagination__ and __[rate limiting](https://www.yelp.com/developers/faq)__ to:\n","\n","1. Retrieve all of the Business objects (# of business objects should equal `total` in the response). Paginate by querying 20 restaurants each request.\n","2. Pause slightly (at least 200 milliseconds) between subsequent requests so as to not overwhelm the API (and get blocked).  \n","\n","As always with API access, make sure you follow all of the [API's policies](https://www.yelp.com/developers/api_terms) and use the API responsibly and respectfully.\n","\n","**DO NOT MAKE TOO MANY REQUESTS TOO QUICKLY OR YOUR KEY MAY BE BLOCKED**\n","\n","Again, you can test your function with an individual neighborhod in Pittsburgh (I recommend Polish Hill). Pittsburgh itself has a lot of restaurants... meaning it will take a lot of time to download them all.\n","\n","```python\n",">>> data = all_restaurants('Polish Hill, Pittsburgh')\n",">>> print(len(data))\n","77\n",">>> print([x['name'] for x in data])\n","['Church Brew Works', \"Salem's Market & Grill\", 'Poulet Bleu', 'Morcilla', ...]\n","```"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"nMxBlzAbM31B"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of restaurants: 83\n","Restaurant names:\n","['Poulet Bleu', 'Morcilla', 'The Forge', 'Kaibur Coffee', \"Salem's Grill\", 'Church Brew Works', 'Driftwood Oven', \"Lola's Eatery\", 'Senti Restaurant', 'The Vandal', 'Umami', 'Piccolo Forno', 'El Sabor Latin Kitchen', 'Condado Tacos', 'Cobra', \"Burgh'ers Brewing\", 'Merchant Oyster Company', 'Z-Best BBQ', \"Zarra's A Taste of Southern Italy\", 'Geppetto Cafe', 'Ineffable Ca Phe', 'Long Story Short', 'Caffe Mona La Bistro', 'The Nook', 'The Hungry CowGirl Cocina', 'Industry Public House', 'Round Corner Cantina', \"Edgar's Best Tacos\", 'Baby Loves Tacos', 'Thai Table & Kitchen', 'Spirits & Tales', 'Pane e Pronto', 'Redfin Blues', 'Tamarind', 'Smoke', 'Pita My Shawarma', 'Dijlah Hookah Lounge', 'All India Authentic cuisine', 'Taste of India', 'Deli on Butler Street', \"Tram's Kitchen\", 'Over Eden', 'Smallman Street Deli', \"Grandma B's\", 'Brillobox', 'Mezcal Mexican Cantina', 'China House', 'Los Cabos Mexican Restaurant', \"Fazio's Pizza\", 'Black Lotus Pizza', 'William Penn Tavern', 'India on Wheels', 'Pizza Pronto Oakland', \"Frankie's Extra Long\", 'Thunderbird Cafe', 'Salonika Imports', 'La Vostra Pizza', \"Preeti's PiTT\", \"Graziano's Pizzeria\", 'Thai Gourment', \"K & T's Fish and Chicken\", \"Salem's Cafe and Event Center\", 'Bridges Restaurant and Lounge', 'Pgh Crepes', 'New China Inn', \"Wendy's\", 'El Pariente Taco Shop', \"Abby's\", \"Tazza D'oro\", 'Chaai and Chaatwala ', 'Pizza Bellagio', 'Hotbox by Wiz', \"Sylvia's Market & Deli\", 'Cafe 1923', 'Subway Restaurants', 'Moonlight Express', 'Forever Rosemary', 'Catatouille', 'Simply Burgers & Fries', 'Hundred Miles Crepes', 'Chick-fil-A', 'Steel City Pizza Fest', \"Olive's Mediterranean Gyro House\"]\n"]}],"source":["def all_restaurants(query):\n","    \"\"\"\n","    Retrieve ALL the restaurants on Yelp for a given query.\n","\n","    Args:\n","        query (string): Search term\n","\n","    Returns:\n","        results (list): list of dicts representing each business\n","    \"\"\"\n","    # Yelp API endpoint for business search\n","    url = \"https://api.yelp.com/v3/businesses/search\"\n","\n","    # Set up headers with the API key\n","    headers = {\n","        'Authorization': f'Bearer {api_key}',\n","    }\n","\n","    offset = 0\n","    results = []\n","\n","    while True:\n","        # Set up parameters for the search query with pagination\n","        params = {\n","            'location': query,\n","            'categories': 'restaurants',\n","            'limit': 20,  # Yelp API limits the count to 20 per request\n","            'offset': offset,\n","            'radius': 1500,\n","        }\n","\n","        # Make the request to the Yelp API\n","        response = requests.get(url, headers=headers, params=params)\n","\n","        if response.status_code == 200:\n","            \n","            data = response.json()\n","\n","            businesses = data.get('businesses', [])\n","            results.extend(businesses)\n","\n","            # Update the offset for the next page\n","            offset += 20\n","\n","            # Check if there are more pages\n","            if offset >= data.get('total', 0):\n","                break\n","        else:\n","            # If the request was not successful, print the error code and response text\n","            print(f\"Error {response.status_code}: {response.text}\")\n","            break\n","        # Pause to avoid overwhelming the API and getting blocked (rate limiting)\n","        time.sleep(0.2)\n","\n","    return results\n","\n","\n","api_key = 'SncupRpadsRXrFICNWIGi-EVxGx48lDI8RtXn93qGrA17cDiMf2iXBJ9AbFaV9a0kwZh91X1EDLWUVKHOI2lk8c5eykO7IvUTvZEwVRnSKjgmvgL1w621sZ87nZ_ZXYx'\n","query = 'Polish Hill, Pittsburgh'\n","\n","# Make the Yelp API request to retrieve all restaurants\n","restaurants_data = all_restaurants(query)\n","\n","# Print the results\n","print(\"Total number of restaurants:\", len(restaurants_data))\n","print(\"Restaurant names:\")\n","print([x['name'] for x in restaurants_data])"]},{"cell_type":"markdown","metadata":{"id":"Lij0iPH7M31C"},"source":["---\n","\n","Now that we have the metadata on all of the restaurants in Pittsburgh (or at least the ones listed on Yelp), we can retrieve the reviews and ratings. The Yelp API gives us aggregate information on ratings but it doesn't give us the review text or individual users' ratings for a restaurant. For that we need to turn to web scraping, but to find out what pages to scrape we first need to parse our JSON from the API to extract the URLs of the restaurants.\n","\n","In general, it is a best practice to seperate the act of __downloading__ data and __parsing__ data. This ensures that your data processing pipeline is modular and extensible (and autogradable). This decoupling also solves the problem of expensive downloading but cheap parsing (in terms of computation and time)."]},{"cell_type":"markdown","metadata":{"id":"HnrqVpeJM31C"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"3ZEOm3zYM31C"},"source":["## Working with Web Pages (and HTML)\n","\n","Think of APIs as similar to accessing a application's database itself (something you can interactively query and receive structured data back). But the results are usually in a somewhat raw form with no formatting or visual representation (like the results from a database query). This is a benefit _AND_ a drawback depending on the end use case. For data science and _programatic_ analysis this raw form is quite ideal, but for an end user requesting information from a _graphical interface_ (like a web browser) this is very far from ideal since it takes some cognitive overhead to interpret the raw information. And vice versa, if we have HTML it is quite easy for a human to visually interpret it, but to try to perform some type of programmatic analysis we first need to parse the HTML into a more structured form.\n","\n","As a general rule of thumb, if the data you need can be accessed or retrieved in a structured form (either from a bulk download or API) prefer that first. But if the data you want (and need) is not as in our case we need to resort to alternative (messier) means.\n","\n","Going back to the \"hello world\" example of question 1 with the NYT, we will do something similar to retrieve the HTML of the Yelp site itself (rather than going through the API) programmatically as text.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"ViT46o2EM31C"},"source":["## Q3: Parse a Yelp restaurant Page\n","\n","Using `BeautifulSoup`, parse the HTML of a single Yelp restaurant page to extract the reviews in a structured form as well as the total number of pages.  A call to this function could look like the following\n","\n","```python\n","reviews, num_pages = parse_yelp_page(\"https://www.yelp.com/biz/the-porch-at-schenley-pittsburgh\")\n","```\n","\n","You will want to fill in the following function stubs to parse a single page of reviews and return:\n","* the reviews as a structured Python dictionary\n","* the total number of pages of reviews.\n","\n","For each review be sure to structure your Python dictionary as follows (to be graded correctly). The order of the keys doesn't matter, only the keys and the data type of the values:\n","\n","```python\n","{\n","    'author': 'Aaron W.' # str\n","    'rating': 4          # int\n","    'date': '2019-01-03' # str, yyyy-mm-dd\n","    'description': \"Wonderful!\" # str\n","}\n","```\n","\n","Return reviews in the order that they are present on the page.  There can be issues with Beautiful Soup using various parsers, for maximum conpatibility (and fewest errors) initialize the library with the default (and Python standard library parser): `BeautifulSoup(markup, \"html.parser\")`. You may notice that the HTML is automatically generated. Yelp uses a modern web application technology called [React](https://reactjs.org/), which generates the markup from Javascript code. This is a common hazard of scraping data from HTML, because the resulting code is not actually that readable.\n","\n","#### Hints:\n","1. As a general strategy, you should try using the Chrome browser (others probably have similar features), and the \"View -> Developer -> Inspect Elements\" command.  This lets you mouse over individual elements in the text and find the corresponding html entry.\n","\n","2. Use the `.find()` and `.find_all()` commands in BeautifulSoup liberally.  In addition to finding exact tags, e.g. `element.find(\"tag\")` you can specify search terms in addition, including with regular exprssions.  For example, to find all sub-elements that have  `class = \" raw__<something>\"` you can use `element.find(\"tag\", class_=re.compile(r\"raw\"))`.\n","\n","3. You can also search the element with a regular expression using `element.find(\"tag\", string=re.compile(r\"<regex>\")`\n","\n","4. You can get the text content of a tag, converted to text (and preserving newlines), using `element.get_text(\"\\n\")`\n","\n","#### Static downloads\n","\n","Finally, we have learned through extensive experience that requiring a class to parse live content pages over a 2 week assignment nearly _always_ will lead to a situation where people add and delete reviews during the assignment period, causing the grader tests to go out of date.  There are also sometimes issues in actually getting the HTML properly from headless nodes like the ones that run on Colab.  Thus, while you would normally, as the first line in your function, issue a command like the following:\n","```python\n","html = retrieve_html(url)[1]\n","```\n","we are also providing a `parse_yelp_page_dict` dictionary along with the assigment that has all the necessary pages (correct as of the assignment release).  So instead of the line above you could call:\n","\n","```python\n","html = parse_yelp_page_dict[url]\n","```\n","These are literally just downloads of the Yelp page HTML as of the time the assignment is released.  While it will be nice to test your function using the real Yelp page, to see it working in action, you probably should do your final testing and submission using these pre-downloaded files.  But note that regardless of whether you use these saved files or the live web version, you will definitely need to use a web browser on the real system to inspect the elements of the live page.  Attempting to manually parse the HTML as it's is downloaded to these files would be extremely difficult."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"rPh2zL8tM31D"},"outputs":[{"ename":"NameError","evalue":"name 'retrieve_html' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[3], line 46\u001b[0m\n\u001b[0;32m     42\u001b[0m     num_pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(pagination_element\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)) \u001b[38;5;28;01mif\u001b[39;00m pagination_element \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reviews, num_pages\n\u001b[1;32m---> 46\u001b[0m reviews, num_pages \u001b[38;5;241m=\u001b[39m \u001b[43mparse_yelp_page\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://www.yelp.com/biz/the-porch-at-schenley-pittsburgh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews: \u001b[39m\u001b[38;5;124m\"\u001b[39m, reviews)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPage:\u001b[39m\u001b[38;5;124m\"\u001b[39m,num_pages)\n","Cell \u001b[1;32mIn[3], line 28\u001b[0m, in \u001b[0;36mparse_yelp_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03mParse the reviews on a single page of a restaurant.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m        second element: Number of pages total\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# To use downloaded data, replace any command (or any equivalent using requests)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_html\u001b[49m(url)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# with the command:\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# html = parse_yelp_page_dict[url]\u001b[39;00m\n\u001b[0;32m     31\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'retrieve_html' is not defined"]}],"source":["import pickle\n","import gzip\n","\n","# data_dict = {'example_link': 'https://www.yelp.com/biz/the-porch-at-schenley-pittsburgh'}\n","\n","# # Save data to the file\n","# with gzip.open(\"parse_yelp_page_dict.pkl.gz\", \"wb\") as f:\n","#     pickle.dump(data_dict, f)\n","\n","\n","with gzip.open(\"parse_yelp_page_dict.pkl.gz\", \"rb\") as f:\n","    parse_yelp_page_dict = pickle.load(f)\n","\n","def parse_yelp_page(url):\n","    \"\"\"\n","    Parse the reviews on a single page of a restaurant.\n","\n","    Args:\n","        url (string): URL string corresponding to a Yelp restaurant\n","\n","    Returns:\n","        tuple(list, int): a tuple of two elements\n","            first element: list of dictionaries corresponding to the extracted review information\n","            second element: Number of pages total\n","    \"\"\"\n","\n","    # To use downloaded data, replace any command (or any equivalent using requests)\n","    html = retrieve_html(url)[1]\n","    # with the command:\n","    # html = parse_yelp_page_dict[url]\n","    soup = BeautifulSoup(html, \"html.parser\")\n","    reviews = []\n","    review_elements = soup.find_all(\"div\", class_=re.compile(\"yelpfrontend__142228__yelpfrontend__GondolaBizDetails__dynamic\"))\n","    for review_element in review_elements:\n","        review = {}\n","        # review[\"author\"] = review_element.find(\"span\", class_=\"user-display-name\").get_text(strip=True)\n","        # review[\"rating\"] = int(review_element.find(\"span\", class_=re.compile(r\"css-1fdy0l5\")))\n","        review[\"date\"] = review_element.find(\"span\", class_=\"css-chan6m\").get_text(strip=True)\n","        # review[\"description\"] = review_element.find(\"p\", class_=\"comment\").get_text(\"\\n\", strip=True)\n","        reviews.append(review)\n","    pagination_element = soup.find(\"div\", class_=\"pagination-block\")\n","    num_pages = int(pagination_element.find_all(\"a\")[-2].get_text(strip=True)) if pagination_element else 1\n","\n","    return reviews, num_pages\n","\n","reviews, num_pages = parse_yelp_page(\"https://www.yelp.com/biz/the-porch-at-schenley-pittsburgh\")\n","print(\"reviews: \", reviews)\n","print(\"Page:\",num_pages)\n"]},{"cell_type":"markdown","metadata":{"id":"Mi4gTKQ-M31D"},"source":["---\n","\n","## Q 3.5: Extract all of the Yelp reviews for a Single Restaurant\n","\n","So now that we have parsed a single page, and figured out a method to go from one page to the next we are ready to combine these two techniques and actually crawl through web pages!\n","\n","Using `requests`, programmatically retrieve __ALL__ of the reviews for a __single__ business (provided as a parameter). Just like the API was paginated, the HTML paginates its reviews (it would be a very long web page to show 300 reviews on a single page) and to get all the reviews you will need to parse and traverse the HTML. As input your function will receive a URL corresponding to a Yelp business. As output return a list of dictionaries (structured the same as question 3 containing the relevant information from the reviews.\n","\n","Return reviews in the order that they are present on the page.\n","\n","You will need to get the number of pages on the first request and generate the URL for subsequent pages automatically. Use the Yelp website to see how the URL changes for subsequent pages."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"nn0S-40kM31D"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Date: 22 Photos\n","Text: 221 Schenley Dr\n","\n"]}],"source":["\n","def extract_yelp_reviews(yelp_url):\n","    headers = {\n","        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n","    }\n","\n","    try:\n","        # Send an initial request to get the first page\n","        response = requests.get(yelp_url, headers=headers)\n","        response.raise_for_status()  # Raise an exception for bad responses (e.g., 404)\n","\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        # Extract the total number of pages\n","        review_count_elem = soup.find('span', class_='review-count')\n","        if review_count_elem:\n","            total_pages = int(review_count_elem.text.strip().split()[-2])\n","        else:\n","            total_pages = 1\n","\n","    \n","        all_reviews = []\n","\n","        \n","        for page_num in range(0, total_pages):\n","            # Generate the URL for the current page\n","            page_url = f\"{yelp_url}?start={page_num * 20}\"\n","\n","            # Send a request to the current page with headers\n","            page_response = requests.get(page_url, headers=headers)\n","            page_response.raise_for_status()  # Raise an exception for bad responses\n","\n","            page_soup = BeautifulSoup(page_response.text, 'html.parser')\n","\n","            reviews = page_soup.find_all('div', class_='css-kmgt1v')\n","\n","            for review in reviews:\n","                review_data = {\n","                    # 'rating': int(review.find('div', class_='css-1fdy0l5')['title'][0]),\n","                    'date': review.find('span',class_='css-chan6m').text.strip(),\n","                    'text': review.find('span',class_='raw__09f24__T4Ezm') .text.strip()\n","                }\n","                all_reviews.append(review_data)\n","            # print(all_reviews)\n","        return all_reviews\n","\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return None\n","\n","# Example :\n","yelp_url = \"https://www.yelp.com/biz/the-porch-at-schenley-pittsburgh\"\n","reviews = extract_yelp_reviews(yelp_url)\n","\n","if reviews:\n","    for i, review in enumerate(reviews, start=1):\n","        print(f\" Date: {review['date']}\")#Review {i} - Rating: {review['rating']},\n","        print(f\"Text: {review['text']}\\n\")\n","else:\n","    print(\"Failed to retrieve reviews.\")\n"]}],"metadata":{"anaconda-cloud":{},"colab":{"provenance":[{"file_id":"1sifuV8hwI8OWTx2ZLPoYQtmNJ2ThS0zO","timestamp":1702836632188}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
